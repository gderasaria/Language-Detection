{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os,re,nltk\n",
    "from tqdm import tqdm\n",
    "from sklearn import cross_validation, linear_model, metrics \n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import Pipeline,FeatureUnion\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First lets start with loading the test and traning data and pre-processing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Clean and Process text to remove unnecessary characters and text. Split paragraphs into sentences. \n",
    "def pre_process(text):\n",
    "    \n",
    "    text = re.sub(\"(<.+>)\" ,\"\",text) #remove the xml data \n",
    "    text = re.sub(\"\\n+\",\"\\n\",text)  \n",
    "    text = re.sub(\"[(\\d)+:,\\(\\)\\-\\\"\\'+\\\\/\\.\\?!]\",\" \", text) #remove the special characters\n",
    "    text = re.sub(\" +\",\" \",text) \n",
    "    text = re.sub(\"^ \",\"\",text)  #Remove spaces at the start\n",
    "    text = text.split(\"\\n\")\n",
    "    text_sentence_list = []\n",
    "    \n",
    "    for paragraph in text[1:len(text)-1]:                   #Split paragraph into sentences\n",
    "        for sentence in nltk.sent_tokenize(paragraph):\n",
    "            if len(sentence.split(\" \")) > 3 :           #Add sentences that are greater than size 3 \n",
    "                text_sentence_list.append(sentence)         \n",
    "\n",
    "    return text_sentence_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21/21 [00:01<00:00, 10.58it/s]\n"
     ]
    }
   ],
   "source": [
    "path = \"text/txt\"\n",
    "folders_list = os.listdir(path)\n",
    "train_text,train_labels = [],[]\n",
    "\n",
    "#For each language parse text files and process them \n",
    "for i in tqdm(range(len(folders_list))):\n",
    "    text_files = os.listdir(path + \"/\" + folders_list[i])\n",
    "    sentence_count = 0\n",
    "    for text_file in text_files: \n",
    "        text = open(path + \"/\" + folders_list[i] + \"/\" + text_file, encoding= 'utf8').read()\n",
    "        text = pre_process(text)            \n",
    "        if sentence_count <= 500: #Limit number of sentences to 500 \n",
    "            train_text.extend(text)     \n",
    "            train_labels.extend([folders_list[i] for j in range(len(text))])\n",
    "            sentence_count += len(text) \n",
    "        else:\n",
    "            break "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prepare the training data \n",
    "train_data = pd.DataFrame({ 'text' : train_text,\n",
    "                            'language' : train_labels })\n",
    "train_data = train_data.sample(frac=1)\n",
    "x_train,y_train = train_data.drop([\"language\"],axis=1),train_data[\"language\"]\n",
    "# x_train,x_test,y_train,y_test = cross_validation.train_test_split(train_data.drop([\"language\"],axis=1), \n",
    "#                                                                   train_data['language'],test_size = 0.2, \n",
    "#                                                                   random_state = 0 ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load data from the test file and process it \n",
    "test  = pd.read_csv(open(\"storage/europarl.test\",encoding = 'utf8'), sep=\"\\t\", names = ['language','text'])\n",
    "test = test.sample(frac = 1)\n",
    "x_test,y_test = test.drop([\"language\"],axis=1),test[\"language\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's create a few classes that would help us with featuring engineering and creating a pipeline for the process. The text can contain key features like the word lengths, the character variation , sentence length and word counts which vary based on language the text belongs to. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextSelector(BaseEstimator,TransformerMixin):\n",
    "    \"\"\"Takes in the dataframe, outputs the text column\"\"\"\n",
    "    \n",
    "    def __init__(self,key):\n",
    "        self.key = key \n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return X[self.key]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class  TextLengthExtractor(BaseEstimator,TransformerMixin):\n",
    "    \"\"\"Takes the dataframe , extracts the text and outputs the length  of the text\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        pass \n",
    "    \n",
    "    def transform(self, df, y = None):\n",
    "        return np.transpose(np.matrix(df[\"text\"].apply(lambda x: len(x)))) \n",
    "    \n",
    "    def fit(self, df, y = None ):\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WordCountExtractor(BaseEstimator,TransformerMixin):\n",
    "    \"\"\"Takes the dataframe, extracts the text and outputs the word count for the text\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def word_count(self,text):\n",
    "        return len(text.split(\" \"))\n",
    "    \n",
    "    def transform(self, df, y = None):\n",
    "        return np.transpose(np.matrix(df[\"text\"].apply(self.word_count)))\n",
    "    \n",
    "    def fit(self, df, y = None):\n",
    "        return self \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MeanWordLengthExtractor(BaseEstimator,TransformerMixin):\n",
    "    \"\"\"Takes the dataframe, extracts the text and outputs the mean word length for the text\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def mean_length(self,text):\n",
    "        return np.mean([len(word) for word in text.split(\" \")])\n",
    "    \n",
    "    def transform(self, df, y = None):\n",
    "        return np.transpose(np.matrix(df[\"text\"].apply(self.mean_length)))\n",
    "    \n",
    "    def fit(self, df, y = None):\n",
    "        return self \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UniqueCharacterCountExtractor(BaseEstimator,TransformerMixin):\n",
    "    \"\"\"Takes the dataframe, extracts the text and outputs the number of unique characters in the text\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def transform(self, df, y = None):\n",
    "        return np.transpose(np.matrix(df[\"text\"].apply(lambda x: len(set(x)))))\n",
    "    \n",
    "    def fit(self, df, y = None):\n",
    "        return self \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now lets use the created classes to build a pipeline. Let's try and implement RandomForest that works well with multiclass classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline([\n",
    "        ('features', FeatureUnion([ \n",
    "           ('ngram_tf_idf',Pipeline([\n",
    "              ('selector', TextSelector(key = \"text\")),    \n",
    "              ('vectorizer', CountVectorizer( analyzer= \"char\")),\n",
    "              ('transformer',TfidfTransformer())\n",
    "              ])), \n",
    "             ('text_length', TextLengthExtractor()),\n",
    "             (\"word_count\", WordCountExtractor()),\n",
    "             ('mean_word_length', MeanWordLengthExtractor()),\n",
    "             ('unique_character_count', UniqueCharacterCountExtractor())\n",
    "            ])),   \n",
    "        ('clf', RandomForestClassifier())\n",
    "       ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's look at a few of the hyperparameters that we can tweak.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['clf__random_state', 'memory', 'clf__min_impurity_decrease', 'features__ngram_tf_idf__steps', 'clf__verbose', 'features__ngram_tf_idf__transformer__sublinear_tf', 'features__unique_character_count', 'features__ngram_tf_idf__vectorizer__strip_accents', 'features__ngram_tf_idf__vectorizer__stop_words', 'clf__min_weight_fraction_leaf', 'features__ngram_tf_idf__selector', 'clf__max_depth', 'features__ngram_tf_idf__transformer__smooth_idf', 'clf__max_features', 'features__ngram_tf_idf__vectorizer__encoding', 'features__ngram_tf_idf__vectorizer__min_df', 'features__text_length', 'clf__warm_start', 'clf', 'clf__max_leaf_nodes', 'features__ngram_tf_idf__vectorizer__lowercase', 'features__ngram_tf_idf__vectorizer__dtype', 'features__ngram_tf_idf__transformer__use_idf', 'features__ngram_tf_idf__vectorizer__ngram_range', 'clf__min_samples_leaf', 'features__ngram_tf_idf__transformer__norm', 'features__ngram_tf_idf__vectorizer__max_df', 'features__ngram_tf_idf__vectorizer', 'clf__oob_score', 'features__ngram_tf_idf__vectorizer__binary', 'clf__min_impurity_split', 'features__ngram_tf_idf__vectorizer__preprocessor', 'features__ngram_tf_idf__selector__key', 'features__ngram_tf_idf__vectorizer__tokenizer', 'features__n_jobs', 'features', 'clf__n_jobs', 'features__ngram_tf_idf__vectorizer__decode_error', 'features__ngram_tf_idf__transformer', 'steps', 'features__ngram_tf_idf__vectorizer__max_features', 'features__ngram_tf_idf__vectorizer__input', 'features__transformer_list', 'features__ngram_tf_idf__vectorizer__vocabulary', 'clf__bootstrap', 'features__mean_word_length', 'features__ngram_tf_idf__vectorizer__token_pattern', 'features__transformer_weights', 'clf__class_weight', 'features__ngram_tf_idf__memory', 'clf__n_estimators', 'features__word_count', 'clf__min_samples_split', 'features__ngram_tf_idf', 'clf__criterion', 'features__ngram_tf_idf__vectorizer__analyzer'])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe.get_params().keys() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We can vary the ngram range and  ignore high frequency characters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise',\n",
       "       estimator=Pipeline(memory=None,\n",
       "     steps=[('features', FeatureUnion(n_jobs=1,\n",
       "       transformer_list=[('ngram_tf_idf', Pipeline(memory=None,\n",
       "     steps=[('selector', TextSelector(key='text')), ('vectorizer', CountVectorizer(analyzer='char', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', i...n_jobs=1,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False))]),\n",
       "       fit_params=None, iid=True, n_jobs=1,\n",
       "       param_grid={'features__ngram_tf_idf__vectorizer__ngram_range': [(1, 2), (1, 3), (1, 4)], 'features__ngram_tf_idf__vectorizer__max_df': [0.9, 0.95]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hyperparameters = {\n",
    "                   'features__ngram_tf_idf__vectorizer__ngram_range' :  [(1,2),(1,3),(1,4)],\n",
    "                   'features__ngram_tf_idf__vectorizer__max_df' : [0.9,0.95]\n",
    "                  }\n",
    "classifier = GridSearchCV(pipe, hyperparameters,cv=5)\n",
    "classifier.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'features__ngram_tf_idf__vectorizer__max_df': 0.9,\n",
       " 'features__ngram_tf_idf__vectorizer__ngram_range': (1, 4)}"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.refit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = classifier.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy is 0.9365277511042827\n"
     ]
    }
   ],
   "source": [
    "print (\"The accuracy is \"  + str(np.mean(pred == y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's try another pipeline with logistic regression classifier to see if we can see an improvement or not. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe1 = Pipeline([\n",
    "        ('features', FeatureUnion([ \n",
    "           ('ngram_tf_idf',Pipeline([\n",
    "              ('selector', TextSelector(key = \"text\")),    \n",
    "              ('vectorizer', CountVectorizer(ngram_range=(1,4), analyzer= \"char\")),\n",
    "              ('transformer',TfidfTransformer())\n",
    "              ])), \n",
    "             ('text_length', TextLengthExtractor()),\n",
    "             (\"word_count\", WordCountExtractor()),\n",
    "             ('mean_word_length', MeanWordLengthExtractor()),\n",
    "             ('unique_character_count', UniqueCharacterCountExtractor())\n",
    "            ])),   \n",
    "        ('clf', linear_model.LogisticRegression())\n",
    "       ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise',\n",
       "       estimator=Pipeline(memory=None,\n",
       "     steps=[('features', FeatureUnion(n_jobs=1,\n",
       "       transformer_list=[('ngram_tf_idf', Pipeline(memory=None,\n",
       "     steps=[('selector', TextSelector(key='text')), ('vectorizer', CountVectorizer(analyzer='char', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', i...ty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False))]),\n",
       "       fit_params=None, iid=True, n_jobs=1,\n",
       "       param_grid={'features__ngram_tf_idf__vectorizer__ngram_range': [(1, 2), (1, 3), (1, 4)], 'features__ngram_tf_idf__vectorizer__max_df': [0.9, 0.95]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hyperparameters = {\n",
    "                   'features__ngram_tf_idf__vectorizer__ngram_range' :  [(1,2),(1,3),(1,4)],\n",
    "                   'features__ngram_tf_idf__vectorizer__max_df' : [0.9,0.95]\n",
    "                  }\n",
    "classifier = GridSearchCV(pipe1, hyperparameters,cv=5)\n",
    "classifier.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'features__ngram_tf_idf__vectorizer__max_df': 0.9,\n",
       " 'features__ngram_tf_idf__vectorizer__ngram_range': (1, 4)}"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.refit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred2 = classifier.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy is 0.991453812175917\n"
     ]
    }
   ],
   "source": [
    "print (\"The accuracy is \"  + str(np.mean(pred2 == y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "test[\"predicted\"] = pred2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20828"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test[\"language\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's take a look at the misclassified examples. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      language                                               text predicted\n",
      "18484       sk                     Stredomorská strava (rozprava)        fi\n",
      "1916        cs  Zároveň je nanejvýš důležité, aby Barma zaháji...        sk\n",
      "2050        da  Betragter vi nordpolsområdet som en potentiel ...        sv\n",
      "6070        es                            Ciertamente, tenía uno.        fi\n",
      "6111        es  De Jericó a Ramallah yo tardaba normalmente 20...        pt\n",
      "18745       sk                        Zatiaľ ju však ešte nemáte.        sl\n",
      "1786        cs  V konečném důsledku budeme muset přijmout komp...        et\n",
      "4329        el                           Είχαμε διαπραγματεύσεις.        fi\n",
      "1273        cs  Jmenovali se William Meyer, Bernard Starie, Re...        sk\n",
      "14324       nl  Het consumentenrecht moet consumenten daadwerk...        fi\n",
      "2858        da  Their will is the law, not only at home, but a...        en\n",
      "7547        et                             Osaistungjärgu avamine        fi\n",
      "6729        es  Resulta especialmente controvertida su interfe...        it\n",
      "6173        es                 El Reglamento me parece muy claro.        it\n",
      "10997       it  (EN) Signor Presidente, nomi quali Lumumba, Mo...        sk\n",
      "17899       sk  Lisabonskú zmluvu potrebujeme.\\nsk\\t(BG) Pán p...        it\n",
      "18633       sk       Vaša španielčina je obdivuhodná, pán Hannan.        sl\n",
      "16971       ro  Banii europeni”, după cum sunt cunoscute fondu...        it\n",
      "6400        es                        Gracias, señora Morgantini.        fi\n",
      "6971        es           ¿Está el vaso medio lleno o medio vacío?        pt\n",
      "1073        cs                            Byl pro to dobrý důvod.        sl\n",
      "18789       sk                         Áno, také je hospodárstvo.        sl\n",
      "16407       pt            Isso representaria uma economia global.        it\n",
      "1181        cs                               Filipíny (hlasování)        fi\n",
      "6975        es                       ¿Hay sanciones alternativas?        fi\n",
      "6223        es  El siguiente punto es el debate sobre la pregu...        sk\n",
      "1571        cs                   Prohrávám s ideálním kandidátem.        et\n",
      "3265        de                     Die Kommission arbeitet daran.        fi\n",
      "1682        cs                   Společnost prožívá krizi hodnot.        sl\n",
      "3696        de                    Leben bedeutet stets Divergenz.        da\n",
      "...        ...                                                ...       ...\n",
      "14660       nl  Na al die jaren ben ik toch optimistisch geble...        de\n",
      "2667        da  Kommissionen prioriterer Brennerjernbaneprojek...        fi\n",
      "7783        et                     Selline on tänapäeva reaalsus.        fi\n",
      "9326        fr                        J'ai dû protester moi-même.        sl\n",
      "18168       sk                          Musíme uvažovať dlhodobo.        et\n",
      "5122        en         Capital adequacy of financial institutions        fi\n",
      "16744       pt                     Relatório Thyssen (A4-0137/99)        fi\n",
      "1524        cs  Po jejím neúspěchu považuji Lisabonskou smlouv...        sk\n",
      "10806       hu                   Szeretnék valamire rávilágítani.        fi\n",
      "8931        fi  Vuoden 1998 alussa rahastolla oli omia varoja ...        et\n",
      "9528        fr  La directive \"habitat\" impose un objectif expl...        ro\n",
      "18192       sk                           Máte nejaké pripomienky?        fi\n",
      "13021       lv                           Apsveicu, Pagano kundze.        et\n",
      "18401       sk                Pripravuje sa mimoriadnym spôsobom.        fi\n",
      "18530       sk                       To boli pravdepodobne fakty.        sl\n",
      "18048       sk                           Hospodárska kríza je tu.        sl\n",
      "3924        de                          Wir sind Freunde der USA.        da\n",
      "18093       sk                     Je to úspech tohto Parlamentu.        it\n",
      "1096        cs                         Co tedy tato síť obsahuje?        en\n",
      "18232       sk              Naša skupina chcela istú flexibilitu.        et\n",
      "18075       sk                              Je iba spoločná mena.        sl\n",
      "9670        fr                              Majorité requise: 305        et\n",
      "18540       sk                           To predtým nebolo možné.        sl\n",
      "6230        es                      En concreto, pedimos rapidez.        ro\n",
      "9245        fr                       Est-ce vraiment souhaitable?        fi\n",
      "16712       pt   Precisamos de verdadeira solidariedade europeia.        et\n",
      "17256       ro                          Declaraţiile Preşedinţiei        fi\n",
      "18652       sk           Vláda sa teraz o tento konflikt zaujíma.        sl\n",
      "14025       nl  B5-0437/2001 van de leden Sylla en Miranda, na...        sk\n",
      "11607       it  Oggi il numero di persone contagiate dal virus...        sk\n",
      "\n",
      "[178 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "misclassified_text =  test[test[\"language\"] != test[\"predicted\"]] \n",
    "print(misclassified_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sk    44\n",
      "cs    24\n",
      "es    18\n",
      "de    11\n",
      "pt    11\n",
      "da     9\n",
      "it     7\n",
      "pl     7\n",
      "fr     7\n",
      "hu     6\n",
      "et     5\n",
      "nl     5\n",
      "ro     4\n",
      "sv     4\n",
      "lt     4\n",
      "en     4\n",
      "fi     3\n",
      "sl     2\n",
      "lv     2\n",
      "el     1\n",
      "Name: language, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(misclassified_text[\"language\"].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The slovak language has highest misclassifications followed by czech and spanish. Let's see what are they being classified as. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sl    26\n",
      "fi     7\n",
      "cs     4\n",
      "it     3\n",
      "et     3\n",
      "hu     1\n",
      "Name: predicted, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "misclassified_text[misclassified_text[\"language\"] == \"sk\"][\"predicted\"].value_counts() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Slovak is misclassified with slovenian which may indicate similarity in the languages. Similarly, we can see the results for other languages below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sl    7\n",
      "sk    7\n",
      "fi    5\n",
      "et    4\n",
      "en    1\n",
      "Name: predicted, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(misclassified_text[misclassified_text[\"language\"] == \"cs\"][\"predicted\"].value_counts() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fi    7\n",
      "it    4\n",
      "pt    3\n",
      "et    2\n",
      "ro    1\n",
      "sk    1\n",
      "Name: predicted, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(misclassified_text[misclassified_text[\"language\"] == \"es\"][\"predicted\"].value_counts() )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clearly the logistic regression beats the random forest in this task. We can use more complex algorithms like RNNs or an esemble of multiple models but this would come at the cost of training time.  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
